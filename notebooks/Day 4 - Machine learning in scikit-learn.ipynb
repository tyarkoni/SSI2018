{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview of Day 4\n",
    "* Preliminaries\n",
    "* What even is machine learning?\n",
    "* Machine learning in Python: why scikit-learn?\n",
    "* Feature extraction\n",
    "* Feature selection\n",
    "* Estimation\n",
    "* Evaluation\n",
    "* Automation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We'll load scikit-learn modules as we go,\n",
    "# so we can see what we're using.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in our preprocessed data set from Day 3.\n",
    "# You may need to modify the path to the file\n",
    "# depending on where you put it on your computer.\n",
    "data = pd.read_csv('../data/preprocessed_data.csv')\n",
    "\n",
    "# Since we'll be predicting outcomes, let's restrict\n",
    "# to only common ones. It's hard to predict something\n",
    "# we don't have very many training examples of.\n",
    "data = data.groupby('outcome').filter(lambda x: len(x) >= 500)\n",
    "\n",
    "# Let's also do some recoding to make life easier\n",
    "data = data.dropna(subset=['age'])\n",
    "categoricals = ['sex', 'sterilized']\n",
    "data[categoricals] = data[categoricals].fillna('Unknown')\n",
    "\n",
    "# Important, otherwise we have problems later\n",
    "# when we try to concatenate based on index\n",
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What even is machine learning?\n",
    "<img src=\"https://s-media-cache-ak0.pinimg.com/736x/be/fc/cc/befcccae9891a505eabee71f7c808d4d.jpg\" style=\"margin-bottom: 10px;\">\n",
    "* Better name might be \"predictive modeling\"\n",
    "    * In contrast to traditional statistical approach, which might be called \"explanatory modeling\" [$^1$](https://projecteuclid.org/euclid.ss/1009213726),[$^2$](http://jakewestfall.org/publications/Yarkoni_Westfall_choosing_prediction.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Traditional approach begins by positing a _data model_\n",
    "    * e.g., linear regression model: $Y = \\beta_0 + \\beta_1X_1 + \\ldots + \\beta_pX_p + \\epsilon$\n",
    "    * ***\"Assuming my model of how the data arose is correct, and given these parameter estimates, to what extent is variation in Y explained by variation in X?\"***\n",
    "    * Models should be interpretable in order to be useful\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In machine learning, overriding emphasis is just on whether we can accurately predict future Y values\n",
    "    * ***\"Regardless of what the true data model may be or what my parameter estimates look like, does my algorithm give outputs (Y) as close as possible to those of the true model when given the same inputs (X)?\"***\n",
    "    * If the model/algorithm is interpretable, that's a bonus, but not generally important\n",
    "    * Premium placed instead on objective tests of accuracy in predicting new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Getting ahead of ourselves:\n",
    "### Overfitting and in-sample vs. out-of-sample error\n",
    "<img src=\"https://qph.ec.quoracdn.net/main-qimg-17ec84ff3f63f77f6b368f0eb6ef1890\" style=\"margin-bottom: 10px;\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* How do we test for overfitting **before** the new data come in?\n",
    "* By setting aside a fraction of our dataset -- no peeking! -- and pretending it's the future data\n",
    "    * These set-aside data are called the *validation set*\n",
    "* Example: We have 1000 samples in the full dataset. \n",
    "    * Fit the model to 900 samples (the training set)\n",
    "    * Test how accurately the model predicts the remaining 100 samples (the validation set)\n",
    "* This is the basic logic. In practice we often use a more sophisticated version of this called *cross-validation*. We'll return to this idea near the end of the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine learning in Python: why scikit-learn?\n",
    "* There are hundreds of ML packages in Python\n",
    "    * Theano, Tensorflow, orange, Pattern, PyMVPA, etc...\n",
    "* But scikit-learn is dominant\n",
    "    * Elegant, powerful interface\n",
    "    * World-class documentation\n",
    "    * Excellent performance\n",
    "* The exception is deep learning--not supported in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The typical predictive modeling pipeline\n",
    "* Feature extraction/engineering\n",
    "* Feature selection/dimensionality reduction\n",
    "* Model/parameter selection\n",
    "* Estimation\n",
    "* Evaluation\n",
    "* Rinse and repeat ad nauseam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature extraction\n",
    "* Deriving *new* (informative, non-redundant) predictors/features from the features you already have\n",
    "* Or adding entirely new features to the dataset (e.g., web scraping for dog breed stats)\n",
    "* Simple example of feature extraction: Adding a squared term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# set up the plotting space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,6))\n",
    "\n",
    "# estimate a simple linear regression model, plot its predictions\n",
    "linear_model = sm.OLS(data['min_weight'], sm.add_constant(data['min_height']), missing='drop')\n",
    "a, b = linear_model.fit().params\n",
    "axes[0].scatter(data['min_height'], data['min_weight'], s=30)\n",
    "axes[0].set(ylabel=\"Breed weight\", xlabel=\"Breed height\")\n",
    "axes[0].set_title(\"Weight = b0 + b1*Height\", size=20)\n",
    "axes[0].plot([0, 30], [a, a + b*30], lw=3)\n",
    "\n",
    "# now add Height-squared to the model and plot the model predictions\n",
    "quad_data = sm.add_constant(pd.DataFrame({'h':data['min_height'], 'h2':data['min_height']**2}))\n",
    "quadratic_model = sm.OLS(data['min_weight'], quad_data, missing='drop')\n",
    "a, b, c = quadratic_model.fit().params\n",
    "axes[1].scatter(data['min_height'], data['min_weight'], s=30)\n",
    "axes[1].set(ylabel=\"Breed weight\", xlabel=\"Breed height\")\n",
    "axes[1].set_title(\"Weight = b0 + b1*Height + b2*Height^2\", size=20)\n",
    "X = np.linspace(0, 30)\n",
    "axes[1].plot(X, [a + b*x + c*x**2 for x in X], lw=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Getting _all_ the features\n",
    "* Well, probably not all... but a _lot_\n",
    "* How much information can we get out of the original dataset?\n",
    "* From an interpretation-oriented standpoint, maybe not much more\n",
    "* From a machine learning standpoint, we've just scratched the surface\n",
    "* Some things we could add: names, colors, any number of interactions...\n",
    "    * The cost of trying out silly things is much lower\n",
    "    * Multicollinearity is not (much of) a concern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag-of-words approach to handling fur color\n",
    "* If you have a lot of data, it's not always worth thinking deeply about your features\n",
    "* E.g., how should we model fur color?\n",
    "    * \"Black/Tricolor\", \"Calico Point\", \"Brown Brindle/Blue Cream\"\n",
    "* Simple approach: treat color descriptions like a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "* Extract all word tokens (possibly even N-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# How many colors in total?\n",
    "print(data['color'].nunique())\n",
    "\n",
    "# First 20 unique colors in the dataset\n",
    "data['color'].unique()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Applying the bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The CountVectorizer is an estimator that takes a series\n",
    "# of documents (or strings) as input, and returns a count\n",
    "# of every word token found in every document. There's also\n",
    "# a TfidfVectorizer in cases where we want normalized frequency.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer with default parameters.\n",
    "# Some common arguments we might want to experiment with\n",
    "# include min_df and max_df (which exclude words that are\n",
    "# too frequent or infrequent), stop_words (which allows\n",
    "# us to pass in a list of words to ignore), and ngram_range,\n",
    "# which enables us to extract multi-word features.\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# Extract all possible word features from the color list.\n",
    "# Note that this returns a sparse matrix rather than a\n",
    "# numpy array or a pandas DataFrame. A sparse matrix is\n",
    "# a way of representing potentially very large 2-d arrays\n",
    "# very efficiently, because we don't need to allocate\n",
    "# memory for every cell in the array, only those that\n",
    "# have a non-zero value.\n",
    "fur_features = vec.fit_transform(data['color'])\n",
    "print(\"fur_features is an object of type:\", type(fur_features))\n",
    "\n",
    "# After fitting, the names of the features (i.e., the\n",
    "# columns of the sparse matrix returned by fit_transform())\n",
    "# are stored in the estimator itself.\n",
    "feature_names = vec.get_feature_names()\n",
    "\n",
    "# Store in a pandas DF for easier manipulation later.\n",
    "# Note that we convert the sparse array back to a dense\n",
    "# one before loading into pandas. If our dataset were\n",
    "# much bigger, we'd probably want to avoid this step\n",
    "# and just keep working with the sparse matrix.\n",
    "fur_features = pd.DataFrame(fur_features.todense(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look...\n",
    "fur_features.head()\n",
    "# We went from 534 unique color combinations to 37 binary color features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# How many instances of each color?\n",
    "fur_features.sum(axis=0).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Adding interaction terms\n",
    "* We could dummy-code all our categorical variables and then take pairwise products\n",
    "* But if we don't need interpretabilty, there's a simpler hack\n",
    "    * Concatenate all variables for which we want interactions\n",
    "    * Dummy-code the result\n",
    "* Let's cross sex (2 categories), sterilization (2 categories), and breed (1787 categories)\n",
    "    * Could result in up to $2 \\times 2 \\times 1787 = 7148$ new features! But since many of these combinations of categories likely contain 0 pets, it'll probably be fewer than that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Simply concatenate the columns we want--making sure to convert any numeric\n",
    "# columns to string, otherwise the concatenation will fail.\n",
    "data['ssb'] = data['sex'].astype(str) + '_' \\\n",
    "    + data['sterilized'].astype(str) + '_' + data['breed']\n",
    "\n",
    "# How many unique levels?\n",
    "num_levels = data['ssb'].nunique()\n",
    "print(\"Total number of unique values: {}\".format(num_levels))\n",
    "\n",
    "# Now we can dummy-code the result\n",
    "pd.get_dummies(data['ssb']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What else?\n",
    "* Very easy to quickly build up thousands of derivative features in this way\n",
    "* Doesn't mean we shouldn't think deeply about good features\n",
    "    * Often, biggest jumps in performance are achieved by adding entirely new features (e.g., external dog breed data)\n",
    "* Point is try to eke out every bit of signal from what we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature selection/reduction\n",
    "* Not all features are created equal\n",
    "* Just because we created 3,000+ features doesn't mean we need to include them all\n",
    "* Two general approaches:\n",
    "    1. Dimensionality reduction (extract latent signal from observed features)\n",
    "        * Principal component analysis (PCA)\n",
    "        * Independent component analysis (ICA)\n",
    "        * etc.\n",
    "    2. Feature selection (filter out features based on some criterion)\n",
    "        * Keep high-variance features\n",
    "        * Keep best-scoring features (i.e., strongest correlation with outcome)\n",
    "        * Fit a preliminary estimator like lasso that drops some features\n",
    "        * etc.\n",
    "* Supported by the `decomposition` and `feature_selection` modules in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Principal Component Analysis (PCA): The big idea\n",
    "* For $P$ variables, we can plot the observations as a scatter plot in $P$-dimensional space\n",
    "    * E.g., for $P=2$ features, we have a two-dimensional scatter plot with two axes, the X-axis and the Y-axis\n",
    "* In PCA, we *rotate* the dataset in $P$-dimensional space to a *new* set of $P$ axes, called *principal components*, such that\n",
    "    1. The observations are uncorrelated along the principal component axes\n",
    "    2. The principal components are sorted in descending order of variance accounted for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Standardize the two features. It's usually \n",
    "# a good idea to do this prior to running PCA\n",
    "df = data[['min_height', 'min_weight']].dropna()\n",
    "df = (df - df.mean()) / df.std()\n",
    "\n",
    "# compute the PCA using scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(df)\n",
    "\n",
    "# set up the plotting space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,7))\n",
    "\n",
    "# data points in original, unrotated features space\n",
    "axes[0].scatter(df['min_height'], df['min_weight'], s=30)\n",
    "axes[0].set_xlabel(\"Breed height\")\n",
    "axes[0].set_ylabel(\"Breed weight\")\n",
    "axes[0].set_ylim([-2, 3.5])\n",
    "axes[0].set_xlim([-2, 3.5])\n",
    "a, b = (0, pca.components_[0,1]/pca.components_[0,0])\n",
    "axes[0].plot([-1.8, 2.4], [a + b*-1.8, a + b*2.4], lw=3)\n",
    "a, b = (0, pca.components_[1,1]/pca.components_[1,0])\n",
    "axes[0].plot([-.9, .9], [a + b*-.9, a + b*.9], lw=3)\n",
    "\n",
    "# data points in rotated principal component (PC) space\n",
    "rotated = pca.transform(df)\n",
    "axes[1].scatter(-rotated[:,0], rotated[:,1], s=30)\n",
    "axes[1].set_xlabel(\"Principal Component 1\")\n",
    "axes[1].set_ylabel(\"Principal Component 2\")\n",
    "axes[1].set_ylim([-2.5, 4])\n",
    "axes[1].set_xlim([-2.5, 4])\n",
    "axes[1].plot([-2.7, 3.5], [0, 0], lw=3)\n",
    "axes[1].plot([0, 0], [-1.3, 1.3], lw=3);\n",
    "\n",
    "# How much of the total variance is explained by each PC?\n",
    "msg = \"The first principal component alone explains {:.0%} of the total variance.\"\n",
    "print(msg.format(pca.explained_variance_ratio_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* It's clear that the first principal component contains most of the useful information that was shared between Height and Weight\n",
    "* So if we simply drop the second principal component, we've effectively reduced our two features down to one feature with minimal information loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now let's do PCA on our array of over 3000 features\n",
    "* PCA on such a large array would be very computationally demanding and take a long time\n",
    "* So we'll do a faster, approximate version of PCA called *Randomized PCA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# First we need to recode our string column as a set of dummies\n",
    "interaction_dummies = pd.get_dummies(data['ssb'])\n",
    "\n",
    "# Let's concatenate this with the color features\n",
    "# axis=1 indicates that we want to concatenate along\n",
    "# the column axis (axis=0 would append each dataframe\n",
    "# below the last.\n",
    "lotsa_features = pd.concat([fur_features, interaction_dummies], axis=1)\n",
    "\n",
    "# Like most other things in sklearn, decomposition classes\n",
    "# implement the estimator interface. So they have fit() and\n",
    "# predict() methods. Transformers also have a transform()\n",
    "# method. First, we initialize the PCA transformer.\n",
    "# We'll use a \"randomized PCA\" solver that is a speedier\n",
    "# approximation of standard the principal component analysis\n",
    "# (PCA) factorization. We need to specify the number of\n",
    "# components we want at initialization. We'll take the first 100.\n",
    "from sklearn.decomposition import PCA\n",
    "rpca = PCA(100, svd_solver='randomized')\n",
    "\n",
    "# Now we can fit and transform in one step\n",
    "rpca_features = rpca.fit_transform(lotsa_features)\n",
    "\n",
    "# How much of the variance do these components explain?\n",
    "\n",
    "# set up the plotting space\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,7))\n",
    "\n",
    "# scree plot\n",
    "axes[0].plot(rpca.explained_variance_ratio_, lw=3)\n",
    "axes[0].set_title('Scree plot (% variance explained by each PC)', size=20)\n",
    "axes[0].set_xlabel('Principal Component number')\n",
    "axes[0].set_ylabel('% of total variance')\n",
    "\n",
    "# cumulative variance explained\n",
    "axes[1].plot(np.cumsum(rpca.explained_variance_ratio_), lw=3)\n",
    "axes[1].set_title('Cumulative variance explained', size=20)\n",
    "axes[1].set_xlabel('Principal Component number')\n",
    "axes[1].set_ylabel('% of total variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimation\n",
    "* With features in hand, we can now fit some models!\n",
    "* scikit-learn has a bewildering array of models\n",
    "* We'll talk about model selection shortly\n",
    "* First we'll look at one model/algorithm in detail: **K-nearest neighbors (KNN)**\n",
    "    * Easy to understand\n",
    "    * Often competitive with more complicated models\n",
    "    * Example of an algorithm really only used in machine learning context (no *data model*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-nearest neighbors (KNN) algorithm\n",
    "<img src=\"http://en.proft.com.ua/media/science/r_knn_concept.png\" style=\"margin-bottom: 10px;\" width=\"1000px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# some data pre-processing\n",
    "df = data[['min_height','min_weight']]\n",
    "df = (df - df.mean()) / df.std()                # standardize before applying PCA.\n",
    "df['size'] = np.dot(df, pca.components_.T)[:,0] # add PC1 as 'size'.\n",
    "df['outcome'] = data['outcome']                 # append outcome in string form\n",
    "df['y'] = LabelEncoder().fit_transform(data['outcome']) # and integer form.\n",
    "df['age'] = data['age']                         # add the age predictor.\n",
    "df = df.dropna()                                # drop missing values.\n",
    "df = df.loc[df['outcome'] != 'Died',]           # drop 'Died' outcome (too few cases, 0.4%)\n",
    "for v in ['age','size']:                        # standardize the predictors. KNN falters\n",
    "    df[v] = (df[v] - df[v].mean())/df[v].std()  # if predictors on wildly different scales.\n",
    "\n",
    "# define function to compute KNN for a given K and plot the result\n",
    "def knn_plot(k, ax):\n",
    "    knn = neighbors.KNeighborsClassifier(k)     # load the KNN classifier\n",
    "    knn.fit(df[['size','age']], df['y'])        # fit the KNN model\n",
    "    predicted = knn.predict(df[['size','age']]) # retrieve the outcomes predicted by KNN\n",
    "    acc = accuracy_score(df['y'], predicted)    # compute accuracy of those predictions\n",
    "    \n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    h = .1  # step size in the mesh\n",
    "    x_min, x_max = df['size'].min() - 1, df['size'].max() + 1\n",
    "    y_min, y_max = df['age'].min() - 1, df['age'].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Create color maps\n",
    "    cmap_bold = ListedColormap(['red', 'green', 'blue', 'yellow'])\n",
    "    cmap_light = ListedColormap(['pink', 'lightgreen', 'lightblue', 'lightyellow'])\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(df['size'], df['age'], c=df['y'], cmap=cmap_bold)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('Breed size (standardized)')\n",
    "    ax.set_ylabel('Dog age (standardized)')\n",
    "    ax.set_title('K = {}; Accuracy = {:.1%}'.format(k, acc), size=20)\n",
    "    \n",
    "# set up the plotting space\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14,14/3))\n",
    "\n",
    "# fit the models and make the plots\n",
    "for k, ax in zip([2, 100, 2000], axes):\n",
    "    knn_plot(k, ax)\n",
    "axes[1].annotate(\n",
    "    'Red = adoption, Green = euthanasia, Blue = transfer, Yellow = return to owner',\n",
    "    xy=(0.5, 0), xytext=(0, -1),\n",
    "    xycoords=('axes fraction', 'figure fraction'),\n",
    "    textcoords='offset points',\n",
    "    size=20, ha='center', va='bottom');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Is this good?\n",
    "* Seems good, no?\n",
    "* There are 4 classes, so chance should be 25%, right?\n",
    "* Or should it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Show the relative class frequencies\n",
    "counts = df['outcome'].value_counts()\n",
    "counts/counts.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We can (probably) do better\n",
    "* Let's use some of that giant mess of features we extracted!\n",
    "* Also, the analysis above only used the dogs (which are about half the dataset), so let's use the full dataset\n",
    "* Finally, it's really easy to plug in other classifiers, so let's try some of those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# For models with categoricals, we can call on\n",
    "# patsy to dummy-code our variables, like we did\n",
    "# when working with statsmodels\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Some other estimators we can try (there are many more!)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC                      # support vector classifier\n",
    "from sklearn.naive_bayes import BernoulliNB      # Naive Bayes for dummy predictors\n",
    "from sklearn.linear_model import RidgeClassifier # ridge regression + classification rule\n",
    "\n",
    "# plug in estimator of choice (just using default arguments)\n",
    "# est = neighbors.KNeighborsClassifier() # default K = 5\n",
    "est = RandomForestClassifier()\n",
    "# est = DecisionTreeClassifier()\n",
    "# est = SVC()\n",
    "# est = BernoulliNB()\n",
    "# est = RidgeClassifier()\n",
    "\n",
    "# grab the outcome and encode it as integer rather than string\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(data['outcome'])\n",
    "\n",
    "# Set up the features, starting with 'age'\n",
    "X = data['age']\n",
    "X = np.c_[X.values, rpca_features[:,:20]]\n",
    "# X = np.c_[X.values, lotsa_features]\n",
    "for i in range(X.shape[1]): # standardize all predictors\n",
    "    X[:,i] = (X[:,i] - X[:,i].mean())/X[:,i].std()\n",
    "\n",
    "# fit the model/algorithm\n",
    "est.fit(X, y)\n",
    "y_predicted = est.predict(X)\n",
    "\n",
    "# We're assessing accuracy *in the training set* so be skeptical!\n",
    "accuracy_score(y, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "* Evaluating model performance is rarely straightforward\n",
    "* There are many criteria we might value\n",
    "* Simple answers can be misleading\n",
    "* Let's take a look at _how_ we classified different outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Confusion matrix\n",
    "* How does the classifier go wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(y, y_predicted):\n",
    "    ''' take true and predicted scores and plot confusion matrix '''\n",
    "    # Get the confusion matrix\n",
    "    cm = confusion_matrix(y, y_predicted)\n",
    "\n",
    "    # Normalize the confusion matrix by dividing each row by its sum\n",
    "    ncm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Put into DataFrame and get all labels from the encoder\n",
    "    class_labels = encoder.classes_\n",
    "    ncm = pd.DataFrame(ncm, index=class_labels, columns=class_labels)\n",
    "\n",
    "    # Rows are true classes, columns are assigned classes\n",
    "    sns.heatmap(data=ncm, fmt='.2f', annot=True, cmap='Blues')\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "plot_confusion_matrix(y, y_predicted)\n",
    "plt.gca().set_title('Confusion matrix\\nRow = reality, Column = decision', size=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The classification report shows us performance for\n",
    "# the most common metrics, by class\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y, y_predicted))\n",
    "print('Precision: Of all cases predicted to be X, how many really were X?')\n",
    "print('Recall: Of all cases that really are X, how many were correctly predicted to be X?\\n')\n",
    "\n",
    "# which integer goes with which outcome?\n",
    "# a cross-tab is a simple way to find out\n",
    "print(pd.crosstab(data['outcome'], y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "* Hopefully performance now looks reasonable\n",
    "* But there's still a potential problem: overfitting\n",
    "* We're training and evaluating on the same dataset--this is a big no-no!\n",
    "* scikit-learn provides easy ways to evaluate models out-of-sample\n",
    "    * This is known as cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import KFold cross-validation\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# Placeholder for scores from each fold\n",
    "scores = []\n",
    "\n",
    "# Create k folds (in our case 5). Loop over the folds,\n",
    "# and for each one, split the dataset into training and test.\n",
    "# In each fold, we train the data on the training values,\n",
    "# and then evaluate its performance on the test. Finally,\n",
    "# we can take the average of the out-of-sample scores as\n",
    "# our estimate of model performance.\n",
    "folds = KFold(n=len(X), n_folds=5, shuffle=True)\n",
    "print(\"Indices of training/test samples in each fold:\")\n",
    "for train, test in folds:\n",
    "    print(train, test) # so we can see what's going on\n",
    "    est.fit(X[train], y[train])\n",
    "    pred_y = est.predict(X[test])\n",
    "    fold_score = accuracy_score(y[test], pred_y)\n",
    "    scores.append(fold_score)\n",
    "    \n",
    "# Overfitting be gone!\n",
    "print(\"\\nAccuracy in each fold:\")\n",
    "print(np.array(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model selection\n",
    "* We've experimented a lot with estimators, decomposition, evaluation, etc.\n",
    "* This is only the tip of the iceberg...\n",
    "* scikit-learn has hundreds of estimators!\n",
    "* Two problems:\n",
    "    1. How are we supposed to choose?\n",
    "    2. How do we do this in a principled way?\n",
    "* Basically we can try different things and see what leads to the highest cross-validation error\n",
    "* Caveat: *ALL* of the steps in our analysis pipeline...\n",
    "    * ...all data pre-processing strategies we try...\n",
    "    * ...all models we try...\n",
    "    * ...all hyperparameters (e.g., K parameter in KNN) we try for each model...\n",
    "* ...should ideally occur within another cross-validation loop, an idea called *nested cross-validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fully automated pipelines\n",
    "* We've done feature extraction, reduction, and selection; estimation; evaluation...\n",
    "* But we can automate this\n",
    "    * Both for efficiency, and to prevent overfitting (greatly facilitates nested cross-validation)\n",
    "* sklearn.pipeline provides functionality for creating [fully automated Pipelines](http://scikit-learn.org/stable/modules/pipeline.html)\n",
    "* We'll build a toy example with 2 steps, but we could chain our entire workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import the Pipeline class\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Initialize a list to store all the steps in our pipeline\n",
    "steps = []\n",
    "\n",
    "# Add feature selection\n",
    "selector = SelectKBest(k=100)\n",
    "steps.append(('select', selector))\n",
    "\n",
    "# Add estimation\n",
    "estimator = LogisticRegression()\n",
    "steps.append(('estimate', estimator))\n",
    "\n",
    "# set up the predictors\n",
    "_X = np.c_[data['age'].values, lotsa_features]\n",
    "for i in range(_X.shape[1]): # standardize all predictors\n",
    "    _X[:,i] = (_X[:,i] - _X[:,i].mean())/_X[:,i].std()\n",
    "\n",
    "# Initialize and fit the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "_y_pred = pipeline.fit(_X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building our own estimator:\n",
    "### The fit-predict interface\n",
    "* scikit-learn is built around the estimator interface\n",
    "* \"An estimator is an object that fits a model based on some training data and is capable of inferring some properties on new data\"\n",
    "* Every estimator must implement fit() and predict() methods\n",
    "* Makes it easy to extend scikit-learn with our own estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class MercurialClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Picks a random class and assigns that label to all cases.\"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ''' Selects a random class from the available options '''\n",
    "        classes = np.unique(y)\n",
    "        self.selected_ = np.random.choice(classes)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        ''' Applies the selected class to everything '''\n",
    "        return np.repeat(self.selected_, len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A very quick dip into deep learning\n",
    "* What is deep learning?\n",
    "    * A field of machine learning that focuses on developing and applying \"deep\" neural network models\n",
    "* Why deep learning?\n",
    "    * The architecture of DNNs is (loosely) modeled on biological neural networks--which are very powerful!\n",
    "    * For many real-world tasks (image recognition, language translation, etc.), deep learning blows everything else out of the water\n",
    "    * A highly technical field\n",
    "    * But also a lot of trial-and-error\n",
    "    * Progress is *extremely* rapid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Deep learning in Python\n",
    "* Python is arguably the premier language for deep learning\n",
    "* Virtually all major frameworks (TensorFlow, Caffe, Torch, etc.) have Python bindings\n",
    "* High-level libraries like `keras` make things even easier\n",
    "* We don't have time to do the topic justice, so let's just see a quick example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Logistic regression with neural networks\n",
    "* Neural networks are extremely powerful function approximators\n",
    "* We can start by training a logistic regression model using a neural net architecture\n",
    "* Then we can start adding complexity (e.g., hidden layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Our y is currently a 1-D array of integers; for multiclass classification,\n",
    "# keras's categorical_crossentropy loss expects a matrix of binary columns.\n",
    "# We use keras's to_categorical utility (which does essentially the same)\n",
    "# thing as pandas' get_dummies()) for the conversion.\n",
    "y_binary = to_categorical(y)\n",
    "\n",
    "# Set aside 20% of data as a hold-out test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2)\n",
    "\n",
    "n_classes = y_train.shape[1]\n",
    "\n",
    "# # Multinomial logistic regression\n",
    "model = Sequential([\n",
    "    Dense(n_classes, activation='softmax', input_dim=X.shape[1])\n",
    "])\n",
    "\n",
    "# # Uncomment the next few lines for a deeper version of the model\n",
    "# # that contains a hidden layer\n",
    "# model = Sequential([\n",
    "#     Dense(50, activation='sigmoid', input_dim=X.shape[1]),\n",
    "#     Dense(n_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# Compile the model using a categorical cross-entry loss\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Shuffle the training data\n",
    "perm = np.random.permutation(len(X_train))\n",
    "X_train = X_train[perm]\n",
    "y_train = y_train[perm]\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Fit the model!\n",
    "model.fit(X_train, y_train, nb_epoch=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Going deeper...\n",
    "* If we improved performance non-trivially just by adding one hidden layer, shouldn't we just keep adding more layers?\n",
    "* In theory, perhaps, but in practice, probably not\n",
    "* Why not?\n",
    "    * Adding parameters makes the model harder to train, so we need more data\n",
    "    * There's probably a fundamental limit to how predictable outcomes are given these data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How did we do?\n",
    "* We can evaluate the neural network's performance using the same tools we used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get predicted scores for the hold-out\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# The predictions are still in binary column format, so we need\n",
    "# to squash them back into a vector of integers of class IDs.\n",
    "# We can do this just by taking the max value in each row.\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = (y_test_class == y_pred_class).mean()\n",
    "print(\"Raw accuracy: {:.3f}\\n\".format(accuracy))\n",
    "\n",
    "names = encoder.classes_\n",
    "print(\"\\nClassification report:\\n\")\n",
    "print(classification_report(y_test_class, y_pred_class, target_names=names))\n",
    "\n",
    "print(\"\\nConfusion matrix:\")\n",
    "plot_confusion_matrix(y_test_class, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The End\n",
    "* I hope you enjoyed the course and/or learned something useful!\n",
    "* The links throughout the notebooks contain plenty of further resources\n",
    "* Consider registering for [SciPy 2018](https://scipy2018.scipy.org) (here in July)--they have great [tutorials](https://scipy2018.scipy.org/ehome/299527/711308/)\n",
    "* Feedback/suggestions for improvement is welcome\n",
    "* Please fill out a course evaluation before you leave"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
